{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6814309eb4c44563a7b41ced3324d9bc",
    "deepnote_cell_height": 422.734375,
    "deepnote_cell_type": "markdown",
    "owner_user_id": "66f6c157-2365-4fbd-8a9b-fcac9255eef2",
    "tags": []
   },
   "source": [
    "# Final Project - Motor Vehicle Collisions - Crashes\n",
    "\n",
    "### _Addressing SDG 3.6 Death Rate due to Road Traffic Injuries._\n",
    "\n",
    "##### Table of content:\n",
    "1. [Motivation](#part1)\n",
    "2. [Baisc stats. Let's understand the dataset better.](#part2)\n",
    "3. [Data Analysis.](#part3)\n",
    "4. [Genre.](#part4)\n",
    "5. [Visualizations.](#part5)\n",
    "6. [Discussion.](#part6)\n",
    "7. [Contributions. Who did what?.](#part7)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "68fe5d888afc41db83dd00400fb52a01",
    "deepnote_cell_height": 293.125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## <font color='green'>Part 1</font>: Motivation\n",
    "<a id=part1></a>\n",
    "This section is to clarify the motivation behind the project ,and what we are going to investigate and how we are going to proceed. \n",
    "\n",
    "\n",
    "\n",
    "The past couple years of all our lives have not been the best. COVID-19 pandemic spread havoc, increasing human suffering, destabilizing the global economy and upending the lives of billions of people around the globe. The importance of health and well being and its larger impact on the global machinery and day to day life of people became clearer to everyone through this troubling times.\n",
    "\n",
    "Since as a team we all wanted to make a really impactful project that would provide meaningful insights that could help people make effective changes in their lives, we decided to delve deeper in the subsject of health and well being. After a lot of brainstorming, we decided to further analyze the Motor Vehicle Collisions - Crashes data set in New York.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6aa1b78f13a14c21a84db3584fd9cc3c",
    "deepnote_cell_height": 441.859375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "\n",
    "### Motor Vehicle Collisions dataset and why we want to analyze the dataset\n",
    "\n",
    "Road accidents are responsible for 1.3 million  deaths and 50 million injuries annually all over the world. Road  crashes are also the leading killer of children and young people worldwide, aged five to 29. \n",
    "The Covid 19 pandemic killed about 6 million people in about 2 years. The reason we were able to get some control over the disease was through diligent efforts of scientists and doctors to better understand the disease and through this gained knowledge we were able to defeat the pandemic. The same needs to happen with global road safety. The pain of the families loosing their loved ones is excruciating and we need to put in efforts to better understand global road safety to make sure everyone's loved ones return safely home from their respective journeys.\n",
    "\n",
    "This was our drive to work on this project as we all feel that this is the most pointless way to dies. Also we would like to mention that this project addresses the Sustainable Development Goal 3, regarding \"Good Health and Well-being\", more specifically target 3.6: Reduce road injuries and deaths. As things stand, they are set to cause a further estimated 13 million deaths and 500 million injuries during the next decade according to UN. \n",
    "Road accidents are entirely preventable, and our priority is to investigate causes so that preventive measures can be implemented.\n",
    "\n",
    "There are also other countless disadvantages of car collisions (e.g. disrupt traffic flow, cost resources, wastes fuel ,loss of life). Other than homicides, the fatal incidents with which police have the most contact with the public are fatal traffic collisions. So there is huge potential in building a traffic safety model that can help detect reasons of collisions early and help implement systemic measure to prevent these unfortunate events. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "911836d48ba14ff1b14e220fa37eb209",
    "deepnote_cell_height": 352.296875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Goals for the end user's experience:\n",
    "\n",
    "\n",
    "So the goal for the end user's experience in this analysis is if first of all to give an understanding of the data in depth. Most people would have several expectations and assumptions for these patterns, like the most accident prone hours, night-driving, bad weather conditions, driver negligence  etc. Therefore we want to provide a detailed overview of how the data is distributed along different time perspectives. \n",
    "\n",
    "In particular, we want to show how the distributions have changed over the years, especially contrasting it with Covid-19 year(2220) as this may provide meaningful insights as the road activity was significantly decreased in 2020. This could provide insights the accidents from an population density perspective, where many hope to see more accidents per capita in more crowded regions.\n",
    "\n",
    "Moreover, we also want to show to what extent we can predict the accidents, their number, how many people die, where does the next accident happen and  the most relevant causes for the accident. And finally, we want to give the user a understanding of how the weather impacts these patterns, as a interesting perspective on the subject.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "849088bed75949999aeb484236747c68",
    "deepnote_cell_height": 133.5625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "The [dataset](https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95) was extracted from the [NYC OpenData](https://opendata.cityofnewyork.us). \n",
    "The size of the data set is 384 MB\n",
    "\\\n",
    "Number of rows 1.88M \n",
    "\\\n",
    "Number of variables  is 29\n",
    "\n",
    "In [Part 2: Basics stats](#part2) we will learn more about our dataset and get familier with it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c534da92ae694424a0b7b1878ec931ac",
    "deepnote_cell_height": 86,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## <font color='green'>Part 2</font>: Basic stats. Let's understand the dataset better.\n",
    "<a id=part2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fdf89e734b8540b1902c85aa248fe00a",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### 2.0 : Setup - Load Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "f5f2419f21f24818b6fc1b8f8cfb45a6",
    "deepnote_cell_height": 701,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5338,
    "execution_start": 1652206679365,
    "source_hash": "6b58e369",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas_profiling==3.2.0 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from pandas_profiling==3.2.0) (1.7.3)\n",
      "Requirement already satisfied: htmlmin>=0.1.12 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from pandas_profiling==3.2.0) (0.1.12)\n",
      "Requirement already satisfied: PyYAML>=5.0.0 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from pandas_profiling==3.2.0) (6.0)\n",
      "Requirement already satisfied: seaborn>=0.10.1 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from pandas_profiling==3.2.0) (0.11.2)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from pandas_profiling==3.2.0) (1.21.2)\n",
      "Requirement already satisfied: multimethod>=1.4 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from pandas_profiling==3.2.0) (1.8)\n",
      "Requirement already satisfied: missingno>=0.4.2 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from pandas_profiling==3.2.0) (0.5.1)\n",
      "Requirement already satisfied: visions[type_image_path]==0.7.4 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from pandas_profiling==3.2.0) (0.7.4)\n",
      "Requirement already satisfied: requests>=2.24.0 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from pandas_profiling==3.2.0) (2.27.1)\n",
      "Requirement already satisfied: matplotlib>=3.2.0 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from pandas_profiling==3.2.0) (3.5.0)\n",
      "Requirement already satisfied: pydantic>=1.8.1 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from pandas_profiling==3.2.0) (1.9.0)\n",
      "Requirement already satisfied: jinja2>=2.11.1 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from pandas_profiling==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from pandas_profiling==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from pandas_profiling==3.2.0) (1.3.5)\n",
      "Requirement already satisfied: tangled-up-in-unicode==0.2.0 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from pandas_profiling==3.2.0) (0.2.0)\n",
      "Requirement already satisfied: markupsafe~=2.1.1 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from pandas_profiling==3.2.0) (2.1.1)\n",
      "Requirement already satisfied: phik>=0.11.1 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from pandas_profiling==3.2.0) (0.12.2)\n",
      "Requirement already satisfied: joblib~=1.1.0 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from pandas_profiling==3.2.0) (1.1.0)\n",
      "Requirement already satisfied: networkx>=2.4 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.4->pandas_profiling==3.2.0) (2.6.3)\n",
      "Requirement already satisfied: attrs>=19.3.0 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.4->pandas_profiling==3.2.0) (21.4.0)\n",
      "Requirement already satisfied: imagehash in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.4->pandas_profiling==3.2.0) (4.2.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.4->pandas_profiling==3.2.0) (8.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from matplotlib>=3.2.0->pandas_profiling==3.2.0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from matplotlib>=3.2.0->pandas_profiling==3.2.0) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from matplotlib>=3.2.0->pandas_profiling==3.2.0) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from matplotlib>=3.2.0->pandas_profiling==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from matplotlib>=3.2.0->pandas_profiling==3.2.0) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from matplotlib>=3.2.0->pandas_profiling==3.2.0) (21.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas_profiling==3.2.0) (2021.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from pydantic>=1.8.1->pandas_profiling==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.2.0->pandas_profiling==3.2.0) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from requests>=2.24.0->pandas_profiling==3.2.0) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from requests>=2.24.0->pandas_profiling==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from requests>=2.24.0->pandas_profiling==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from requests>=2.24.0->pandas_profiling==3.2.0) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from tqdm>=4.48.2->pandas_profiling==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: PyWavelets in c:\\users\\tala1\\anaconda3\\lib\\site-packages (from imagehash->visions[type_image_path]==0.7.4->pandas_profiling==3.2.0) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -otebook (c:\\users\\tala1\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otebook (c:\\users\\tala1\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otebook (c:\\users\\tala1\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otebook (c:\\users\\tala1\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otebook (c:\\users\\tala1\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otebook (c:\\users\\tala1\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas_profiling==3.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "e785c733f2a5474dbae3d288c4cafeb0",
    "deepnote_cell_height": 1053,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5136,
    "execution_start": 1652206684716,
    "source_hash": "b1bf16f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import random as ran\n",
    "import collections\n",
    "import math\n",
    "import functools\n",
    "import operator\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import tree\n",
    "import plotly.express as px \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('fivethirtyeight') # For better style\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, plot_confusion_matrix\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import calendar\n",
    "import plotly.graph_objects as go  \n",
    "from plotly.subplots import make_subplots # creating subplots\n",
    "from sklearn.metrics import r2_score\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'holidays'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25352/4249200570.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mholidays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchart_studio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchart_studio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchart_studio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotly\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'holidays'"
     ]
    }
   ],
   "source": [
    "import holidays\n",
    "from xgboost import XGBRegressor\n",
    "import chart_studio\n",
    "import chart_studio.tools as tls\n",
    "import chart_studio.plotly as py\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap\n",
    "import folium\n",
    "from pandas_profiling import ProfileReport\n",
    "from sklearn.linear_model import LinearRegression;\n",
    "from sklearn.linear_model import RidgeCV,Lasso;\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,BaggingRegressor,GradientBoostingRegressor;\n",
    "from sklearn.tree import DecisionTreeRegressor;from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cc7ddbfe522c447aaf7bd3a1759f9f0b",
    "deepnote_cell_height": 54,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### Setup - Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "0367e57bd3d04099ad79755140da551f",
    "deepnote_cell_height": 225,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1652206689866,
    "source_hash": "f160f7fe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def push_viz(fig, name):\n",
    "    username = 'tato' # your username\n",
    "    api_key = 'g6OEy1bn0ffMLRNu0Wvq' # your api key - go to profile > settings > regenerate key\n",
    "    chart_studio.tools.set_credentials_file(username=username, api_key=api_key)\n",
    "\n",
    "    #Push your visualiztion to your account using the following lines of code:\n",
    "    py.plot(fig, filename = name, auto_open=True)\n",
    "\n",
    "    tls.get_embed('https://plotly.com/~tato/1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "464e4639d0aa4704996148be91d1a510",
    "deepnote_cell_height": 225,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8,
    "execution_start": 1652206689875,
    "source_hash": "dc6bb388",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def push_viz2(fig, name):\n",
    "    username = 'talaaz' # your username\n",
    "    api_key = 'djesapBwV1FfBzb35N6l' # your api key - go to profile > settings > regenerate key\n",
    "    chart_studio.tools.set_credentials_file(username=username, api_key=api_key)\n",
    "\n",
    "    #Push your visualiztion to your account using the following lines of code:\n",
    "    py.plot(fig, filename = name, auto_open=True)\n",
    "\n",
    "    tls.get_embed('https://plotly.com/~talaaz/1/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7bd993358b4f44348a96a4d7f7eb19a1",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### 2.1: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "173c6053028c4e4d9e18438e2a3c7a46",
    "deepnote_cell_height": 81,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8079,
    "execution_start": 1652206689924,
    "source_hash": "fddf2924",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/tala1/Downloads/Motor_Vehicle_Collisions_-_Crashes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3a9fb61c94fb4ced83e186f3452c5043",
    "deepnote_cell_height": 329.90625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### 2.2:  Data Cleaning and Preprocessing\n",
    "\n",
    "\n",
    "The Motor Vehicle Collisions crash dataset contains details on the crash event. Each row represents a crash event. The Motor Vehicle Collisions data  contain information from all police reported motor vehicle collisions in NYC. The police report (MV104-AN) is required to be filled out for collisions where someone is injured or killed, or where there is at least $1000 worth of damage.\n",
    "\n",
    "The dataset has 29 columns for each data point mainly-  CRASH DATE\t,CRASH TIME\t, BOROUGH, ZIP CODE,LATITUDE, LONGITUDE, LOCATION, ON STREET NAME, CROSS STREET NAME, OFF STREET NAME, NUMBER OF PERSONS INJURED, NUMBER OF PERSONS KILLED, NUMBER OF PEDESTRIANS INJURED, NUMBER OF PEDESTRIANS KILLED, NUMBER OF CYCLIST INJURED, NUMBER OF CYCLIST KILLED, NUMBER OF MOTORIST INJURED, NUMBER OF MOTORIST KILLED, CONTRIBUTING FACTOR VEHICLE 1-5\t(5 columnsof reasons for accident)2, COLLISION_ID(Unique record id), VEHICLE TYPE CODE 1-5(vehicle types involved in the crash- like bicycle,car/SUV).\n",
    "\n",
    "Lets look at some samples of the dataset to get a better undersatnding of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "58ee3df9107a498bb815764b05713f15",
    "deepnote_cell_height": 412,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 52,
    "execution_start": 1652206698087,
    "source_hash": "c085b6ba",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b2ffcee61f2344d18b9a873c30e5213c",
    "deepnote_cell_height": 223.125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "As you can see that there are a lot of NAN values is colums like Cross Street , Off street etc but since we dont use these columns in our analysis and just use latitude and longitude and borough(district) for geographical analysis of the dataset so we dont need to delete these NAN rows as these columns are removed in our analysis anyway. The columns that are being dropped are-'ZIP CODE','ON STREET NAME','CROSS STREET NAME', and 'OFF STREET NAME'. \n",
    "But since we are keeping latitude and longitude and borough(district), rows with NAN value in these columns are removed.\n",
    "\n",
    "The dataset has data from 2010 to May 2022. In order to reduce the size of data, use the most relevant recent information and contrast road data of Non-Covid years with Covid years, we are filtering for years from 2018 to 2021.We leave out 2022 too as its an incomplete data year. We have also changed the CRASH DATA to datetime datatype to make the processing easier. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "64671ddfc8004a308eace1f4dc7526b0",
    "deepnote_cell_height": 315,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3069,
    "execution_start": 1652206698135,
    "source_hash": "89fb3a06",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Change the formate of date \n",
    "df[\"CRASH DATE\"]= pd.to_datetime(df[\"CRASH DATE\"])\n",
    "df[\"CRASH DATE\"] =  df['CRASH DATE'].dt.date\n",
    "#Preprocess the data based on year 2018 to 2021\n",
    "mask = (df['CRASH DATE'] >= pd.to_datetime(\"2018-01-01\")) & (df['CRASH DATE'] < pd.to_datetime(\"2021-01-01\"))\n",
    "df = df.loc[mask]\n",
    "\n",
    "\n",
    "#Drop nan values of location\n",
    "df=   df[df['LATITUDE']. notna()]\n",
    "df=   df[df['LONGITUDE']. notna()]\n",
    "df['BOROUGH']=  df['BOROUGH'].fillna('')\n",
    "#drop columns\n",
    "df =df.drop(columns=['ZIP CODE','ON STREET NAME','CROSS STREET NAME','OFF STREET NAME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6b7672a9f962432c85c85d7c8535f366",
    "deepnote_cell_height": 97.171875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "As you can see there are 5 columns with Contributing Factor and  Vehicle, each corresponding to the 5 Vehicle Type for each vehicle involved in the accident. Lets look at the number of missing values in these columns to better gauge the number of vehicles usually involved in an accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "76e0cbe1ec6041349cad188b9ddb8c8b",
    "deepnote_cell_height": 1030.875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 740,
    "execution_start": 1652206701204,
    "source_hash": "1e51b377",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cf_vtc= df[[\"CONTRIBUTING FACTOR VEHICLE 1\",\"CONTRIBUTING FACTOR VEHICLE 2\",\"CONTRIBUTING FACTOR VEHICLE 3\", \"CONTRIBUTING FACTOR VEHICLE 4\", \"CONTRIBUTING FACTOR VEHICLE 5\", \"VEHICLE TYPE CODE 1\",\"VEHICLE TYPE CODE 2\",\"VEHICLE TYPE CODE 3\", \"VEHICLE TYPE CODE 4\", \"VEHICLE TYPE CODE 5\" ]]\n",
    "\n",
    "print(\"\\n\\n Contributing Factor Vehicle Columns Stats \\n\\n\")\n",
    "print(\"Total Number of rows with CONTRIBUTING FACTOR VEHICLE 1=nan      -  \",df[\"CONTRIBUTING FACTOR VEHICLE 1\"].isna().sum() )\n",
    "print(\"Total Number of rows with CONTRIBUTING FACTOR VEHICLE 2=nan      - \",df[\"CONTRIBUTING FACTOR VEHICLE 2\"].isna().sum() )\n",
    "print(\"Total Number of rows with CONTRIBUTING FACTOR VEHICLE 3=nan      -\",df[\"CONTRIBUTING FACTOR VEHICLE 3\"].isna().sum() )\n",
    "print(\"Total Number of rows with CONTRIBUTING FACTOR VEHICLE 4=nan      -\",df[\"CONTRIBUTING FACTOR VEHICLE 4\"].isna().sum() )\n",
    "print(\"Total Number of rows with CONTRIBUTING FACTOR VEHICLE 5=nan      -\",df[\"CONTRIBUTING FACTOR VEHICLE 5\"].isna().sum() )\n",
    "    \n",
    "\n",
    "print(\"\\n\\n Vehicle Type Code Columns Stats \\n\\n\")\n",
    "print(\"Total Number of rows with VEHICLE TYPE CODE 1=nan                -  \",df[\"VEHICLE TYPE CODE 1\"].isna().sum() )\n",
    "print(\"Total Number of rows with VEHICLE TYPE CODE 2=nan                -\",df[\"VEHICLE TYPE CODE 2\"].isna().sum() )\n",
    "print(\"Total Number of rows with VEHICLE TYPE CODE 3=nan                -\",df[\"VEHICLE TYPE CODE 3\"].isna().sum() )\n",
    "print(\"Total Number of rows with VEHICLE TYPE CODE 4=nan                -\",df[\"VEHICLE TYPE CODE 4\"].isna().sum() )\n",
    "print(\"Total Number of rows with VEHICLE TYPE CODE 5=nan                -\",df[\"VEHICLE TYPE CODE 5\"].isna().sum() )\n",
    "                  \n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Total Number of rows with one nan(Either CF or VTC) in  column   -\",  df_cf_vtc.isna().any(axis=1).sum()) \n",
    "print(\"Total Number of rows with atleast one nan in any column          -\",  df.isna().any(axis=1).sum())  \n",
    "print(\"Total Number of rows in the dataframe                            -\",df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1bd73f79355d46fd8254ea0ec5f677f1",
    "deepnote_cell_height": 214.734375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "As it can be seen above that the number of rows with atleast one nan in Contributing factor columns and number of rows when CONTRIBUTING FACTOR VEHICLE 5=nan is the same(553736). Also the number of nan's in the dataframe decrease as we go from  CONTRIBUTING FACTOR VEHICLE 5 to CONTRIBUTING FACTOR VEHICLE 1. This means that the police officers fill in these columns from 1-5 depending on the number of vehicles involved in the accident. Same applies to Vehicle type code as well. \n",
    "\n",
    "From this insight, we can create two new columns, one with the number of vehicles in the accident and two with the combined contributing factors. \n",
    "\n",
    "Lets first clean the contributing factors columns and map them on a dictionary for easy combing and processing of this column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "cf1d94d521504c0781728e1df7b5bf83",
    "deepnote_cell_height": 351,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2739,
    "execution_start": 1652206702012,
    "source_hash": "cc328920",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"CONTRIBUTING FACTOR VEHICLE 2\"]= df[\"CONTRIBUTING FACTOR VEHICLE 2\"].replace(\"Reaction to Other Uninvolved Vehicle\",\"Reaction to Uninvolved Vehicle\")\n",
    "df[\"CONTRIBUTING FACTOR VEHICLE 2\"]= df[\"CONTRIBUTING FACTOR VEHICLE 3\"].replace(\"Reaction to Other Uninvolved Vehicle\",\"Reaction to Uninvolved Vehicle\")\n",
    "df[\"CONTRIBUTING FACTOR VEHICLE 2\"]= df[\"CONTRIBUTING FACTOR VEHICLE 4\"].replace(\"Reaction to Other Uninvolved Vehicle\",\"Reaction to Uninvolved Vehicle\")\n",
    "df[\"CONTRIBUTING FACTOR VEHICLE 2\"]= df[\"CONTRIBUTING FACTOR VEHICLE 5\"].replace(\"Reaction to Other Uninvolved Vehicle\",\"Reaction to Uninvolved Vehicle\")\n",
    "\n",
    "df_cf= df[[\"CONTRIBUTING FACTOR VEHICLE 1\",\"CONTRIBUTING FACTOR VEHICLE 2\",\"CONTRIBUTING FACTOR VEHICLE 3\", \"CONTRIBUTING FACTOR VEHICLE 4\", \"CONTRIBUTING FACTOR VEHICLE 5\" ]]\n",
    "\n",
    "cf_dict= dict(enumerate(df[\"CONTRIBUTING FACTOR VEHICLE 1\"].unique()))\n",
    "inv_map = {v: k for k, v in cf_dict.items()} \n",
    "\n",
    "df_cf=df_cf.replace({\"CONTRIBUTING FACTOR VEHICLE 1\": inv_map})\n",
    "df_cf=df_cf.replace({\"CONTRIBUTING FACTOR VEHICLE 2\": inv_map})\n",
    "df_cf=df_cf.replace({\"CONTRIBUTING FACTOR VEHICLE 3\": inv_map})\n",
    "df_cf=df_cf.replace({\"CONTRIBUTING FACTOR VEHICLE 4\": inv_map})\n",
    "df_cf=df_cf.replace({\"CONTRIBUTING FACTOR VEHICLE 5\": inv_map})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "734ef16e62574b76800c319c819ba986",
    "deepnote_cell_height": 74.78125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Now we will aggregate the information in Vehic and CONTRIBUTING FACTOR VEHICLE columns into two columns- one with vehicle count and the other being contributing factors list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "c51ebced6f694a349aef48764d78c755",
    "deepnote_cell_height": 981,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 201690,
    "execution_start": 1652206705211,
    "source_hash": "9db2bc8f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "Vehicle_count=[]\n",
    "contri_fac=[]\n",
    "Car_killed=[]\n",
    "Car_injured=[]\n",
    "df_check=df.isnull() \n",
    "\n",
    "for index, row in df_check.iterrows():\n",
    "    c=0\n",
    "    ck= df.loc[index, 'NUMBER OF PERSONS KILLED']-df.loc[index, 'NUMBER OF PEDESTRIANS KILLED']-df.loc[index, 'NUMBER OF CYCLIST KILLED']-df.loc[index, 'NUMBER OF MOTORIST KILLED']\n",
    "    ci= df.loc[index, 'NUMBER OF PERSONS INJURED']-df.loc[index, 'NUMBER OF PEDESTRIANS INJURED']-df.loc[index, 'NUMBER OF CYCLIST INJURED']-df.loc[index, 'NUMBER OF MOTORIST INJURED']\n",
    "    Car_killed.append(ck)\n",
    "    Car_injured.append(ci)\n",
    "    \n",
    "    if(row['VEHICLE TYPE CODE 1']==False):\n",
    "         c=c+1\n",
    "    if(row['VEHICLE TYPE CODE 2']==False):\n",
    "         c=c+1\n",
    "    if(row['VEHICLE TYPE CODE 3']==False):\n",
    "         c=c+1\n",
    "    if(row['VEHICLE TYPE CODE 4']==False):\n",
    "         c=c+1\n",
    "    if(row['VEHICLE TYPE CODE 5']==False):\n",
    "         c=c+1\n",
    "    Vehicle_count.append(c)\n",
    "    \n",
    "    cf=[]\n",
    "    \n",
    "    if(row['CONTRIBUTING FACTOR VEHICLE 1']==False):\n",
    "         cf.append(int(df_cf.loc[index, 'CONTRIBUTING FACTOR VEHICLE 1']))\n",
    "    if(row['CONTRIBUTING FACTOR VEHICLE 2']==False):\n",
    "         cf.append(int(df_cf.loc[index, 'CONTRIBUTING FACTOR VEHICLE 2']))\n",
    "    if(row['CONTRIBUTING FACTOR VEHICLE 3']==False):\n",
    "         cf.append(int(df_cf.loc[index, 'CONTRIBUTING FACTOR VEHICLE 3']))\n",
    "    if(row['CONTRIBUTING FACTOR VEHICLE 4']==False):\n",
    "         cf.append(int(df_cf.loc[index, 'CONTRIBUTING FACTOR VEHICLE 4']))\n",
    "    if(row['CONTRIBUTING FACTOR VEHICLE 5']==False):\n",
    "         cf.append(int(df_cf.loc[index, 'CONTRIBUTING FACTOR VEHICLE 5']))\n",
    "    contri_fac.append(sorted(list(dict.fromkeys(cf))))\n",
    "    \n",
    "df.insert(1, \"NUMBER OF PERSONS IN CARS KILLED\", Car_killed)\n",
    "df.insert(1, \"NUMBER OF PERSONS IN CARS INJURED\", Car_injured)\n",
    "\n",
    "\n",
    "df.insert(1, \"Vehicle Count\", Vehicle_count)\n",
    "vtc= [ \"VEHICLE TYPE CODE 1\",\"VEHICLE TYPE CODE 2\",\"VEHICLE TYPE CODE 3\", \"VEHICLE TYPE CODE 4\", \"VEHICLE TYPE CODE 5\" ]\n",
    "df =df.drop(columns=vtc)\n",
    "\n",
    "\n",
    "df.insert(1, \"Combined Contributing Factors\", contri_fac)\n",
    "cf= [\"CONTRIBUTING FACTOR VEHICLE 2\",\"CONTRIBUTING FACTOR VEHICLE 3\", \"CONTRIBUTING FACTOR VEHICLE 4\", \"CONTRIBUTING FACTOR VEHICLE 5\" ]\n",
    "df =df.drop(columns=cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "14bd2c3aa878490098f2b378f7768d9b",
    "deepnote_cell_height": 99,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1652206906935,
    "source_hash": "9b4a124",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ki=['NUMBER OF PEDESTRIANS INJURED','NUMBER OF PEDESTRIANS KILLED','NUMBER OF CYCLIST INJURED','NUMBER OF CYCLIST KILLED','NUMBER OF MOTORIST INJURED','NUMBER OF MOTORIST KILLED']\n",
    "# df =df.drop(columns=ki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f265d9c0cd1443b6adf70e568ab673ee",
    "deepnote_cell_height": 117,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1652206906935,
    "source_hash": "f75de6ad",
    "tags": []
   },
   "outputs": [],
   "source": [
    "del(df_cf)\n",
    "del(df_cf_vtc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "92f28e6fadc646f092fb2d5112a59341",
    "deepnote_cell_height": 74.78125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Now we have the two new columns with vehicle count and combined contributing factors. By doing the above aggregation we were able to eliminate a lot of missing values and managed to condense the columns into useful information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "516ce3f6bd8d4045aeb4d6fbd3547a56",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### 2.3: Reformat the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cfc66a5ef25c4685a42d3cae4e384a0e",
    "deepnote_cell_height": 119.5625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Further to increase ease of use, we are pre-processing the data to create columns like Year, Month Time, Day Of Week Number, Day Of Week, Hour of the day,  Time of day in percent of the day passed, and Hour of the week in a seperate dataframe. This was done to  make the plotting and processing of time series for different time periods easier in the analysis and we used a new dataframe to keep the original one in mint condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f708d78a1e944b67a5f5b9e353e8dcf0",
    "deepnote_cell_height": 333,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3935,
    "execution_start": 1652206906936,
    "source_hash": "b4d741b3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "crash_data = df\n",
    "\n",
    "## Create column of datetime object\n",
    "crash_data['DateTime'] = pd.to_datetime(crash_data['CRASH DATE']) \n",
    "## Create column of datetime year \n",
    "crash_data['Year'] = crash_data['DateTime'].apply(lambda x: x.year)\n",
    "crash_data['Time'] = pd.to_datetime(crash_data['CRASH TIME'], format='%H:%M')\n",
    "crash_data['DayOfWeekNumber']= pd.DatetimeIndex(crash_data['DateTime']).weekday\n",
    "crash_data['DayOfWeek']= crash_data['DateTime'].dt.day_name()\n",
    "crash_data['hourofday']= crash_data['Time'].dt.hour\n",
    "crash_data['minute']= crash_data['Time'].dt.minute\n",
    "crash_data['timeofdaypercent'] = crash_data['hourofday'] +(crash_data['minute']/60) \n",
    "crash_data['month']= pd.DatetimeIndex(crash_data['DateTime']).month_name()\n",
    "crash_data['hourofweek']=  crash_data['hourofday'] + (crash_data['DayOfWeekNumber']*24)\n",
    "crash_data['weekofyear']= df['DateTime'].dt.week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "c15681ecbe5b44868c26bf897ded7fa3",
    "deepnote_cell_height": 495,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 239,
    "execution_start": 1652206910897,
    "source_hash": "2555e2ad",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Given order=\n",
    "weekday = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "df_weekday= crash_data.groupby('DayOfWeek').size().reset_index(name=\"Count\")\n",
    "df_weekday['DayOfWeek'] = pd.Categorical(df_weekday['DayOfWeek'], categories=weekday, ordered=True)\n",
    "df_weekday = df_weekday.sort_values('DayOfWeek')\n",
    "\n",
    "df_hourofday= crash_data.groupby('hourofday').size().reset_index(name=\"Count\")\n",
    "df_hour_of_week= crash_data.groupby('hourofweek').size().reset_index(name=\"Count\")\n",
    "df_weekofyear= crash_data.groupby('weekofyear').size().reset_index(name=\"Count\")\n",
    "\n",
    "\n",
    "df_year= crash_data.groupby('Year').size().reset_index(name=\"Count\")\n",
    "df_week_year= crash_data.groupby(['Year','weekofyear']).size().reset_index(name=\"Count\")\n",
    "\n",
    "df_month_year= crash_data.groupby(['Year','month']).size().reset_index(name=\"Count\")\n",
    "df_month_year['month'] = pd.Categorical(df_month_year['month'], categories=months, ordered=True)\n",
    "df_month_year = df_month_year.sort_values('month')\n",
    "\n",
    "\n",
    "df_month= crash_data.groupby('month').size().reset_index(name=\"Count\")\n",
    "df_month['month'] = pd.Categorical(df_month['month'], categories=months, ordered=True)\n",
    "df_month = df_month.sort_values('month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4c59386a590f44a497b158ef5060613d",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "\n",
    "### 2.4: Understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ff52752780a3431ba29caf3460640be0",
    "deepnote_cell_height": 133.171875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### 2.4.1. How Frequently Are Measurements Done?\n",
    "\n",
    "The Motor Vehicle Collisions data tables contain information from all police reported motor vehicle collisions in NYC. The police report (MV104-AN) is required to be filled out for collisions where someone is injured or killed, or where there is at least $1000 worth of damage (https://www.nhtsa.gov/sites/nhtsa.dot.gov/files/documents/ny_overlay_mv-104an_rev05_2004.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0e0c016a0d3d4dc8b1302c2df060a507",
    "deepnote_cell_height": 54,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### 2.4.2 Pandas Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ba8bf14d126f465f95c3d7cbdae84920",
    "deepnote_cell_height": 99,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 12,
    "execution_start": 1652206911137,
    "source_hash": "adc4aa32",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# profile = ProfileReport(df, title=\"Pandas Profiling Report\")\n",
    "# profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "813930120dc6477bbc31431df77f2d85",
    "deepnote_cell_height": 81,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3284709,
    "execution_start": 1652206911150,
    "source_hash": "bd1e2ba0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# profile.to_file(\"basic_stats.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1f2b680c729f429f8fa07f4bc8c52969",
    "deepnote_cell_height": 86,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## <font color='green'>Part 3</font>: Data Analysis\n",
    "<a id=part3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "797dde81e4c140b982c1a27b6da267cf",
    "deepnote_cell_height": 141.953125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "In this section, we will analyse the data in more detail. We will start by investigating the temporal patterns of the data - exploring the number of accidents by day of week, hour of the day, month, and hour of the week and year. Then we will look at a distribution of the accidents across different regions in the city on a heatmap. After that we will look at the accidents across districts in terrms of how many people died, people injured, motorist died, motorist injured, pedestrian died, pedenstrian injured, cyclist died and cyclist injured. We further provide a interactable visualization to explore the district on the death and injury tolls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e37cda8b4b244e0bb25b26c1195a4bf8",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### 3 3.1. Investigating Temporal Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9e6f7a3432b94491a1410fbeb1d28330",
    "deepnote_cell_height": 290.90625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "In this first part of the data analysis, we'll look into the temporal patterns of the data to investigate questions like:\n",
    "\n",
    "1. At which days during the week do accidents happen  the most?\n",
    "2. What hour of day people is most accident prone? \n",
    "3. Is there a month or season where accidents happen more?\n",
    "4. Has there been a change from 2018-2020? Has Covid-19 led to reduced accidents?\n",
    "\n",
    "For the purpose of answering these questions,  we use visualizations of aggregate accidents counts over time to make patterns more noticable. We found similar figures on plotting deaths/injured people count as well reaffirming common logic that more accidents lead to more people dying and getting injured. We show one plot for comparison for the viewer in the accidents by day of the week section. We believe that the visualizations would make it easier to gauge patterns over time, rather than trying to analyze a bunch of numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "76c0d7f0a0014baeb5a0efe357c0a99e",
    "deepnote_cell_height": 54,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### 3.3.3.1 Hourly Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8840d127ff8c4d97bd91ef4b3c299f85",
    "deepnote_cell_height": 97.171875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "So the first pattern we want to investigate is the hourly patterns during a day. We want to examine how many accidents happen over hour of day. We wanted to see if patterns appear from looking at the total number of accidents happening over hour of day just over the entire time frame of 3 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f4af1b61e7984319b3b0521e724a01cb",
    "deepnote_cell_height": 750,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     527
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1254,
    "execution_start": 1652206911178,
    "source_hash": "8740e05",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.bar(df_hourofday, x='hourofday', y=\"Count\", title='Number of accidents by hour of day', color=\"hourofday\", text='Count')\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_xaxes(title='hours of day')\n",
    "fig.update_yaxes(title='Number of accidents')\n",
    "fig.update_traces(hovertemplate='Hour: %{x} <br>Number of accidents: %{y}')\n",
    "fig.show()\n",
    "\n",
    "#push_viz(fig,'accidentsbyHourofDay') #pushing the plot to plotly servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3540742c7aa94077abca74762507d750",
    "deepnote_cell_height": 164.34375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "The visualization above reveals some interesting results and reaffirms a lot of what we were expecting. The accidents number is low in the night hours except a minor peak at midnight. This is probably because of people sleeping and less people being on roads. Also it can be seen that the hours of 16-17 have the global peak, where most accidents happen in the day. This could be due to several reasons- peak traffic times , tiredness after a long day, more rushing to home mentality etc. We also found that contrary to popular beliefs, night driving is generally much safer than day driving as less accidents happen in the night hours, probably due to less people on the roads. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2d70dfb557d9453d8c488d27f7593d66",
    "deepnote_cell_height": 54,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### 3.3.3.2 Weekly Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0116296d44dc464ebd536028218ad5e7",
    "deepnote_cell_height": 111.171875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "In this section we explore the accident count based on a weekly time scale. First we look at plainly the number of accidents based on day of week aggregated over all years. Then we will look at accident count over the hour of week it happened in. \n",
    "\n",
    "First, lets  inspect the accident count based on day of week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "210774b3a2a24a7dbc5c1845ed3a0c95",
    "deepnote_cell_height": 750,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 575,
    "execution_start": 1652206912486,
    "source_hash": "4b21287b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.bar(df_weekday, x='DayOfWeek', y=\"Count\", title='Number of accidents by Weekday', color='DayOfWeek',text='Count')\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_xaxes(title='Weekdays')\n",
    "fig.update_yaxes(title='Number of accidents')\n",
    "fig.update_traces(hovertemplate='Day: %{x} <br>Number of accidents: %{y}')\n",
    "fig.show()\n",
    "\n",
    "#push_viz(fig,'accidentsbyweekday') #pushing the plot to plotly servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "acea6fd6ceac49c8b85c1311e26598cb",
    "deepnote_cell_height": 192.34375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Here we clearly see that accidents happen less on the weekends. The accidents number are pretty consistent from Monday to Thursday but peak in the week on Friday. Our theory is that this is probably because people are most rushed on this day to get to home from work. If this is true, then we should see more accidents happening on Fridays during 15-17 in the week. \n",
    "\n",
    "In order to test our theory and further find some more weekly patterns. We split the week into 168 hours, starting from 00:00 on Monday as hour 0 and ending with midnight on Sunday.\n",
    "\n",
    "Lets loook at the accident count by hour of the week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6c6411fcfe61492a9569dd669096a453",
    "deepnote_cell_height": 750,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     527
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 454,
    "execution_start": 1652206912648,
    "source_hash": "9e3842e9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.bar(df_hour_of_week, x='hourofweek', y=\"Count\", title='Number of accidents by hour of week', color=\"hourofweek\",text=\"Count\")\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_xaxes(title='Hour of week')\n",
    "fig.update_yaxes(title='Number of accidents')\n",
    "fig.update_traces(hovertemplate='Hour of week: %{x} <br>Number of accidents: %{y}')\n",
    "fig.show()\n",
    "\n",
    "#push_viz(fig,'accidentsbyhourofweek') #pushing the plot to plotly servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "34ae5f04af1a4903aaa50e351b24f1d0",
    "deepnote_cell_height": 200.734375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Looking at the visualization above, we do see a peak of 5966 accidents  in the 112 hour of the week which comes to be 16h on Friday. The second maximum is on Friday to at 17h with 5924 accidents. The third maximum is on Thurday at 17 h with 5812 accidents followed by Thursday at 16h aith 5733 accidents.\n",
    "\n",
    "These results indicate that Friday evenings are most dangerous times to drive between 16-18 followed by any working day betweeen the same hours. This indicates that  people should be especially careful when on the road and show more patience especially on Fridays. The lack of patience and accident number is due to high traffic volumes but this reiterates the greater need to especially be  calm when under stress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "dd1ea83d02e841aa8d5eaed22b3a4d98",
    "deepnote_cell_height": 54,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### 3.3.3.3 Monthly Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bd0c026aee344151a35806e0be99168d",
    "deepnote_cell_height": 97.171875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Now, we enlarge the time-perspective a bit and look at the monthly and seasonal patterns. We want to address the question of whether the month/season has an impact on how many accidents are there chooses to bicycle. To analyze this, lets first look at the number of accidents over the month of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ccc538bd5ceb4109ade7d1fe92472309",
    "deepnote_cell_height": 750,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     527
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10,
    "execution_start": 1652206913093,
    "source_hash": "3db401c7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.bar(df_month, x='month', y=\"Count\", title='Number of accidents by month', color=\"month\", text='Count')\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_xaxes(title='months')\n",
    "fig.update_yaxes(title='Number of accidents')\n",
    "fig.update_traces(hovertemplate='Hour: %{x} <br>Number of accidents: %{y}')\n",
    "fig.show()\n",
    "\n",
    "#push_viz(fig,'accidentsbymonth') #pushing the plot to plotly servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2991c00ab8eb46ae805692fdfc3ec65a",
    "deepnote_cell_height": 155.953125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "This visualization shows differences between accidents based on month. Most accidents happen in January. There is a slight dip in Februrary but that could also be because the number of days are less in Februrary. There is a significant dip in April which is suprising. Also the end of the year November and December has less accidents. This could  be because of less people driving in the snow in the winters. \n",
    "\n",
    "Lets look at the accident count by week of year to look deeper into the monthly patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "7c5367d152614d939f3dbb5e041e3082",
    "deepnote_cell_height": 750,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     527
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 9,
    "execution_start": 1652206913095,
    "source_hash": "d67f5036",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.bar(df_weekofyear, x='weekofyear', y=\"Count\", title='Number of accidents by week of year', color=\"weekofyear\",text=\"Count\")\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_xaxes(title='Week of year')\n",
    "fig.update_yaxes(title='Number of accidents')\n",
    "fig.update_traces(hovertemplate='Week of year: %{x} <br>Number of accidents: %{y}')\n",
    "fig.show()\n",
    "\n",
    "#push_viz(fig,'accidentsbyweekofyear') #pushing the plot to plotly servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "dc4dc44f80744d1eb3be9e3cc160f154",
    "deepnote_cell_height": 237.125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "From the visualization, it appears that the last two weeks in December are verry low for accidents. This could be because of the holiday season and people being happy and relaxed and more considerate of other drivers. The last week of the year has the lowest accidents by far which could additionally because of people being at home with their families and less traffic on roads.\n",
    "\n",
    "We also observe peaks starting the 1st week of January and peaking around mid january. This could be a result of people being more ambitious towards their new year goals and more rushed apart from other reasons.\n",
    "\n",
    "The most suprising is the dip around week 13-15. This could be attributed to spring and Easter holidays. Another thing that might be skewing numbers for April is the strict lockdown announced in April 2020 due to the COVID-19 virus. To investigate this further we will look at yearly patterns in the next section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9f7442892f0a422db9a0aadcddf7bce5",
    "deepnote_cell_height": 54,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### 3.3.3.4 Yearly Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fdbd075797834064a8ca9c1a5b769120",
    "deepnote_cell_height": 97.171875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Finally we want to investigate whether there seem to be any patterns in how many accidents are there  over the years. Here we are keen to look at the effect of COVID-19 and the lockdown on the number of accidents that happened and the contrast with non-lockdown years. For this we first visualize the number of accidents over the three years to see if can see any pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f61d63d478654ea899ba6335f9f67938",
    "deepnote_cell_height": 750,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 75,
    "execution_start": 1652206913096,
    "source_hash": "cda054f8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.bar(df_year, x='Year', y=\"Count\", title='Number of accidents by year', color=\"Year\", text='Count')\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_xaxes(title='Year')\n",
    "fig.update_yaxes(title='Number of accidents')\n",
    "fig.update_traces(hovertemplate='Year: %{x} <br>Number of accidents: %{y}')\n",
    "fig.show()\n",
    "\n",
    "#push_viz(fig,'accidentsbyyear') #pushing the plot to plotly servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "254cc647b6074bbfaa2e44ae7848c61e",
    "deepnote_cell_height": 133.5625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    " From the plot we observe that there was a decrerase in the number of accidents going from 2018 to 2019 which is due to a general increase in road safety over years. Unsuprisingly 2020, saw a huge drop with the number of accidents almost falling to half. This was probably due to the COVID-19 pandemic and people staying and working from home and the lockdowns.\n",
    "\n",
    " Lets investigate further by looking at monthly patterns over years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "c988cbb841554a14afe697cb70a31a84",
    "deepnote_cell_height": 750,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     527
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 193,
    "execution_start": 1652206913370,
    "source_hash": "7713a473",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.bar(df_month_year, x='month', y=\"Count\", title='Number of accidents by month by year', facet_col=\"Year\",color=\"month\", text='Count', barmode=\"group\")#,category_orders={\"Year\": [2018, 2019,2020]})\n",
    "fig.update_layout(showlegend=False,barmode='group')\n",
    "fig.update_xaxes(title='months')\n",
    "fig.update_traces(hovertemplate='month: %{x} <br>Number of accidents: %{y}')\n",
    "fig.show()\n",
    "\n",
    "#push_viz(fig,'accidentsbymonthbyyear') #pushing the plot to plotly servers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d97cfd733eb842cdb863a3609e32fdab",
    "deepnote_cell_height": 223.125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "There is a lot to unpack in the plot above. Firstly we can see that 2020-the COVID year has relatively low activity over the entire year in comparison to the other two years. Secondly its importnat to note how there is always a dip in the number of accidents for April which is suprising. This could be attributed to Easter Holidays but we are not sure. Thirdly, Februrary sees a dip probably due to lower number of days except in 2020 when the effect of lockdown in March has reduced the accident numbers. Lastly, and most importantly, there is a trend of general decrease in the number of accidents month over month which is a good sign meaning people are getting more aware of road safety.\n",
    "\n",
    "Finally, lets look at the number of accidents over week over years to better see weekly deviations in accident patterns over these three years in New York:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a72087f55e1c4952a781dc83ac272069",
    "deepnote_cell_height": 732,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     527
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 156,
    "execution_start": 1652206913566,
    "source_hash": "6274239c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.bar(df_week_year, x='weekofyear', y=\"Count\", title='Number of accidents by week of year by year', color=\"weekofyear\",facet_col=\"Year\",text=\"Count\")\n",
    "fig.update_layout(showlegend=False,barmode='group')\n",
    "fig.update_xaxes(title='Week of year')\n",
    "fig.update_traces(hovertemplate='Week of year: %{x} <br>Number of accidents: %{y}')\n",
    "fig.show()\n",
    "\n",
    "#push_viz(fig,'accidentsbyweekofyearbyyear') #pushing the plot to plotly servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "35f9ce4662644a6fae5ae8cc3a83df1d",
    "deepnote_cell_height": 237.125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "From the above plot, firstly we would like to point out the dip starting around week 12 and dropping completely in in week 15 in 2020. Thats around end of March and April. This was the time the details about the new virus were coming out and lockdowns were being imposed so this is good the effect is seen in the data as well.\n",
    "\n",
    "Secondly,we see a peak around week 25-26 which is about in June when the weather is good and more people are out on the roads. Even during the pandemic year higher levels of activity can be seen in the summer months. \n",
    "\n",
    "Lastly we observe a dip in December which is probably due to the holiday season. From these visualization, it seems like holidays is a good time for road safety. This is probably due to more relaxed state of minds and a higher than usual desire to get to your loved ones safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9a9cc24dcc9b4242a8bfb911a489c2ed",
    "deepnote_cell_height": 225,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1648734,
    "execution_start": 1652206913773,
    "source_hash": "335aff",
    "tags": []
   },
   "outputs": [],
   "source": [
    "del(df_month)\n",
    "del(df_hourofday)\n",
    "del(df_weekday)\n",
    "del(df_hour_of_week)\n",
    "del(df_month_year)\n",
    "del(df_week_year)\n",
    "del(df_year)\n",
    "del(df_weekofyear)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b4cf622c812e49308e47c0c5df4ebb3d",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### 3.3.2. Geographical  Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7096d0ac6b574cec8f39842055ec1117",
    "deepnote_cell_height": 352.6875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "In this  part of the data analysis, we'll look into the geographical  patterns of the data in New York to investigate questions like:\n",
    "\n",
    "1. Are there certain locations more likely to have accidents?\n",
    "2. Are there certain boroughs more likely to have accident? \n",
    "3. Do cyclist die/injured more in some boroughs?\n",
    "4. Do motorist die/injured more in some boroughs?\n",
    "5. Are pedestrians killed/injured more in some boroughs?\n",
    "\n",
    "For the purpose of answering these questions,  we will use a heat map to look at a distribution of the accidents across different regions in the city . After that we will look at the accidents across districts in terms of how many people died, people injured, motorist died, motorist injured, pedestrian died, pedestrian injured, cyclist died and cyclist injured. We further provide a interactable visualization to explore the districts on their death and injury tolls.\n",
    "\n",
    "We believe that the visualizations would make it easier to compare districts by the user depending on the district they live, work or commute through and the mode of transport used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "30babe22ab844e40a341bed0afc7f3cb",
    "deepnote_cell_height": 54,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### 3.3.2.1. Distribution of accidents based on location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b1a3d1d9ee8e43d98fb1c4595a1c65a0",
    "deepnote_cell_height": 119.5625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "So the first pattern we want to investigate here is the distribution of accidents across the states. To examine this, we visualize here a heat map based on the latitude and longitude of the accidents aggregated over the entire time period. Here we just want to see where do accidents happen if we forget the dimension of time for a bit. Lets look at the heat map to see the distribution in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "26d0162e53c24116bf4c585760858108",
    "deepnote_cell_height": 960.59375,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     521.59375
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 91279,
    "execution_start": 1652206913774,
    "source_hash": "39d417dd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "map_hooray = folium.Map(location=[40.7128, -74.0060],tiles=\"Stamen Toner\",zoom_start = 10.5)  \n",
    "\n",
    "\n",
    "# Ensure you're handing it floats\n",
    "df['LATITUDE'] = df['LATITUDE'].astype(float)\n",
    "df['LONGITUDE'] = df['LONGITUDE'].astype(float)\n",
    "\n",
    "# List comprehension to make out list of lists\n",
    "heat_data = [[row['LATITUDE'],row['LONGITUDE']] for index, row in df.iterrows()]\n",
    "\n",
    "# Plot it on the map\n",
    "HeatMap(heat_data, min_opacity=0.20, blur=12, radius=15, max_zoom=1,\n",
    "                   ).add_to(map_hooray)\n",
    "\n",
    "#folium.Marker([40.77542, -64.4034], popup=\" Hall of Justice\", icon=folium.Icon(color=\"red\", icon=\"info-sign\")).add_to(map_hooray)\n",
    "\n",
    "map_hooray.save(\"heatmap.html\")\n",
    "# Display the map\n",
    "map_hooray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9299843235aa49cb935b91427b1282c7",
    "deepnote_cell_type": "text-cell-p",
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "From the visualization above, it can be seen that most accidents happen in Manhattan. This is probably due to higher population density, more work places, more tourists, condensed housing and rasher driving due to more traffic. As you move away from the city center, the accident density is dropping as well on account of cars and population densities. Let's further look at district wise analysis in the next section to look deeper into the accident details of different regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d8f4aeab5b644a1f8b7433974132e73f",
    "deepnote_cell_height": 54,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### 3.3.2.2. District Wise Accidents Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "dabbfc8e0abe4b12b66b6e29dcfae390",
    "deepnote_cell_height": 141.953125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "In this section, we wanted to compare the different boroughs of New York to see ho they are doing in terms of how many people died, people injured, motorist died, motorist injured, pedestrian died, pedestrian injured, cyclist died and cyclist injured. This would help identify the right infrastructure to improve and the right safety measures to implement through additional signs and campaigns targeted based on user and mode of transport. Lets look at the visualization for the districts to better understand this question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a42e1879cee147c785d5a8c61fcdc3e3",
    "deepnote_cell_height": 99,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1582537,
    "execution_start": 1652207005517,
    "source_hash": "f7bf442e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2= df.groupby(['BOROUGH']).sum()[['NUMBER OF PERSONS INJURED']]\n",
    "y = df2[\"NUMBER OF PERSONS INJURED\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a768915e5d144518973f042e4c7cefad",
    "deepnote_cell_height": 1578,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     527
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1464,
    "execution_start": 1652207005517,
    "source_hash": "b63c0a46",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_PERSONSINJURED = ( df.groupby(['BOROUGH']).sum() [\"NUMBER OF PERSONS INJURED\"]).reset_index()\n",
    "df_PERSONSKILLED =( df.groupby(['BOROUGH']).sum() [\"NUMBER OF PERSONS KILLED\"]).reset_index()\n",
    "df_PEDESTRIANSINJURED =( df.groupby(['BOROUGH']).sum() [\"NUMBER OF PEDESTRIANS INJURED\"]).reset_index()\n",
    "df_PEDESTRIANSKILLED =( df.groupby(['BOROUGH']).sum() [\"NUMBER OF PEDESTRIANS KILLED\"]).reset_index()\n",
    "df_PEDESTRIANSINJURED =( df.groupby(['BOROUGH']).sum() [\"NUMBER OF PEDESTRIANS INJURED\"]).reset_index()\n",
    "df_CYCLISTINJURED=( df.groupby(['BOROUGH']).sum() [\"NUMBER OF CYCLIST INJURED\"]).reset_index()\n",
    "df_CYCLISTKILLED =( df.groupby(['BOROUGH']).sum() [\"NUMBER OF CYCLIST KILLED\"]).reset_index()\n",
    "df_MOTORISTINJURED =( df.groupby(['BOROUGH']).sum() [\"NUMBER OF MOTORIST INJURED\"]).reset_index()\n",
    "df_MOTORISTKILLED =( df.groupby(['BOROUGH']).sum() [\"NUMBER OF MOTORIST KILLED\"]).reset_index()\n",
    "\n",
    "fig = make_subplots(rows=4,\n",
    "                    cols=2,\n",
    "                    horizontal_spacing = 0.15,\n",
    "                    vertical_spacing= 0.20,\n",
    "                    subplot_titles=[\"NUMBER OF PERSONS INJURED\",\n",
    "                    \"NUMBER OF PERSONS KILLED\",\n",
    "                    \"NUMBER OF PEDESTRIANS INJURED\" ,\n",
    "                    \"NUMBER OF PEDESTRIANS KILLED\",\n",
    "                    \"NUMBER OF CYCLIST INJURED\",\n",
    "                    \"NUMBER OF CYCLIST KILLED\",\n",
    "                    \"NUMBER OF MOTORIST INJURED\",\n",
    "                    \"NUMBER OF MOTORIST KILLED\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.add_trace(go.Bar(x=df_PERSONSINJURED['BOROUGH'],\n",
    " y=df_PERSONSINJURED['NUMBER OF PERSONS INJURED'], name='NUMBER OF PERSONS INJURED',marker_color='tomato'),row=1, col=1)\n",
    "fig.add_trace(go.Bar(x=df_PERSONSKILLED['BOROUGH'],\n",
    " y=df_PERSONSKILLED['NUMBER OF PERSONS KILLED'], name='NUMBER OF PERSONS KILLED',marker_color='darkred'),row=1,  col=2)\n",
    "\n",
    "\n",
    "fig.add_trace(go.Bar(x=df_PEDESTRIANSINJURED['BOROUGH'],\n",
    " y=df_PEDESTRIANSINJURED[ \"NUMBER OF PEDESTRIANS INJURED\"], name= \"NUMBER OF PEDESTRIANS INJURED\",marker_color='blue'),row=2,  col=1)\n",
    "fig.add_trace(go.Bar(x=df_PEDESTRIANSKILLED['BOROUGH'],\n",
    " y=df_PEDESTRIANSKILLED['NUMBER OF PEDESTRIANS KILLED'], name='NUMBER OF PEDESTRIANS KILLED', marker_color='deepskyblue'),row=2,  col=2)\n",
    "\n",
    "\n",
    "\n",
    "fig.add_trace(go.Bar(x=df_CYCLISTINJURED['BOROUGH'],\n",
    " y=df_CYCLISTINJURED[ \"NUMBER OF CYCLIST INJURED\"], name= \"NUMBER OF CYCLIST INJURED\",marker_color='lightgreen'),row=3,  col=1)\n",
    "fig.add_trace(go.Bar(x=df_CYCLISTKILLED['BOROUGH'],\n",
    " y=df_CYCLISTKILLED['NUMBER OF CYCLIST KILLED'], name='NUMBER OF CYCLIST KILLED',marker_color='darkolivegreen'),row=3,  col=2)\n",
    "\n",
    "\n",
    "fig.add_trace(go.Bar(x=df_MOTORISTINJURED['BOROUGH'],\n",
    " y=df_MOTORISTINJURED[ \"NUMBER OF MOTORIST INJURED\"], name= \"NUMBER OF MOTORIST INJURED\", marker_color='indigo'),row=4,  col=1)\n",
    "fig.add_trace(go.Bar(x=df_MOTORISTKILLED['BOROUGH'],\n",
    " y=df_MOTORISTKILLED['NUMBER OF MOTORIST KILLED'], name='NUMBER OF MOTORIST KILLED', marker_color='darkviolet'),row=4,  col=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.update_layout(title='Accidents district wise',showlegend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "03d34050fc82405ebcb73f962a0fb843",
    "deepnote_cell_height": 259.515625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "From the plot above, it can be seen that Brooklyn has higher numbers of persons injured. Queens has a higher number of pedestrians killed which is making it the leading district in people killed followed by Brooklyn. Brooklyn has a higher number of cyclist killed, motorist killed and motorist injured followed by Queens. Brooklyn leads in cyclist injured followed by Manhattan and Queens. In pedestrians injured, Brooklyn leads as well followed by Queens and Manhattan.\n",
    "\n",
    "From these observations, more measures need to implemented in Brooklyn for road safety for pedestrians, motorist and cyclists in general. Queens needs to seriously need to look into pedestrian safety followed by measures in areas of cyclist and motorist safety as well. Manhattan, though crowded, is still relastively safer but new measures need to implemented to combat harm to cyclist and pedestrians.\n",
    "\n",
    "Further in the next visualization, we provide you an interative visualization based on choice of mode of transport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "be6ea91a5c104db9ab82ac935d8b9eba",
    "deepnote_cell_height": 81,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7,
    "execution_start": 1652207006985,
    "source_hash": "deb14897",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#push_viz(fig,'Accidentsdistrictwise') #pushing the plot to plotly servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "aa0959f9476440f883861ed4afaad4eb",
    "deepnote_cell_type": "text-cell-p",
    "is_collapsed": false,
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9166b4b92def411b914e043ef1d04ddc",
    "deepnote_cell_height": 2226,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     527
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 50,
    "execution_start": 1652207007015,
    "source_hash": "9c721d51",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=df_PERSONSINJURED['BOROUGH'],\n",
    " y=df_PERSONSINJURED['NUMBER OF PERSONS INJURED'], name='NUMBER OF PERSONS INJURED',marker_color='tomato'))\n",
    "fig.add_trace(go.Bar(x=df_PERSONSKILLED['BOROUGH'],\n",
    " y=df_PERSONSKILLED['NUMBER OF PERSONS KILLED'], name='NUMBER OF PERSONS KILLED',marker_color='darkred'))\n",
    "\n",
    "fig.add_trace(go.Bar(x=df_PEDESTRIANSINJURED['BOROUGH'],\n",
    " y=df_PEDESTRIANSINJURED[ \"NUMBER OF PEDESTRIANS INJURED\"], name= \"NUMBER OF PEDESTRIANS INJURED\", marker_color='blue'))\n",
    "fig.add_trace(go.Bar(x=df_PEDESTRIANSKILLED['BOROUGH'],\n",
    " y=df_PEDESTRIANSKILLED['NUMBER OF PEDESTRIANS KILLED'], name='NUMBER OF PEDESTRIANS KILLED', marker_color='deepskyblue'))\n",
    "\n",
    "fig.add_trace(go.Bar(x=df_CYCLISTINJURED['BOROUGH'],\n",
    " y=df_CYCLISTINJURED[ \"NUMBER OF CYCLIST INJURED\"], name= \"NUMBER OF CYCLIST INJURED\", marker_color='lightgreen'))\n",
    "fig.add_trace(go.Bar(x=df_CYCLISTKILLED['BOROUGH'],\n",
    " y=df_CYCLISTKILLED['NUMBER OF CYCLIST KILLED'], name='NUMBER OF CYCLIST KILLED', marker_color='darkolivegreen'))\n",
    "\n",
    "\n",
    "\n",
    "fig.add_trace(go.Bar(x=df_MOTORISTINJURED['BOROUGH'],\n",
    " y=df_MOTORISTINJURED[ \"NUMBER OF MOTORIST INJURED\"], name= \"NUMBER OF MOTORIST INJURED\", marker_color='indigo'))\n",
    "fig.add_trace(go.Bar(x=df_MOTORISTKILLED['BOROUGH'],\n",
    " y=df_MOTORISTKILLED['NUMBER OF MOTORIST KILLED'], name='NUMBER OF MOTORIST KILLED', marker_color='darkviolet'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            active=1,\n",
    "            buttons=list([\n",
    "                dict(label=\"None selected\",\n",
    "                     method=\"restyle\",\n",
    "                     args=[{\"visible\": [False, False, False, False,\n",
    "                     False, False, False, False]},\n",
    "                           {\"title\": \"None selected\"}]),\n",
    "                dict(label=\"NUMBER OF PERSONS INJURED\",\n",
    "                     method=\"restyle\",\n",
    "                     args=[{\"visible\": [True, False, False, False,\n",
    "                     False, False, False, False, False]},\n",
    "                           {\"title\": \"NUMBER OF PERSONS INJURE\"}]),\n",
    "                dict(label= \"NUMBER OF PERSONS KILLED\",\n",
    "                     method=\"restyle\",\n",
    "                     args=[{\"visible\": [False, True, False, False,\n",
    "                     False, False, False, False]},\n",
    "                           {\"title\":  \"NUMBER OF PERSONS KILLED\"}]),\n",
    "                dict(label=\"NUMBER OF PEDESTRIANS INJURED\",\n",
    "                     method=\"restyle\",\n",
    "                     args=[{\"visible\": [False, False, True, False,\n",
    "                     False, False, False, False]},\n",
    "                           {\"title\": \"NUMBER OF PEDESTRIANS INJURED\"}]),\n",
    "                dict(label=\"NUMBER OF PEDESTRIANS KILLED\",\n",
    "                     method=\"restyle\",\n",
    "                     args=[{\"visible\": [False, False, False, True,\n",
    "                     False, False, False, False]},\n",
    "                           {\"title\": \"NUMBER OF PEDESTRIANS KILLED\"}]),\n",
    "                dict(label=\"NUMBER OF CYCLIST INJURED\",\n",
    "                     method=\"restyle\",\n",
    "                     args=[{\"visible\": [False, False, False, False,\n",
    "                     True, False, False, False]},\n",
    "                           {\"title\":\"NUMBER OF CYCLIST INJURED\"}]),\n",
    "                dict(label=\"NUMBER OF CYCLIST KILLED\",\n",
    "                     method=\"restyle\",\n",
    "                     args=[{\"visible\": [False, False, False, False,\n",
    "                     False, True, False, False]},\n",
    "                           {\"title\": \"NUMBER OF CYCLIST KILLED\"}]),\n",
    "                dict(label=\"NUMBER OF MOTORIST INJURED\",\n",
    "                     method=\"restyle\",\n",
    "                     args=[{\"visible\": [False, False, False, False,\n",
    "                     False, False, True, False]},\n",
    "                           {\"title\":\"NUMBER OF MOTORIST INJURED\"}]),\n",
    "                dict(label=\"NUMBER OF MOTORIST KILLED\",\n",
    "                     method=\"restyle\",\n",
    "                     args=[{\"visible\": [False, False, False, False,\n",
    "                     False, False, False, True]},\n",
    "                           {\"title\": \"NUMBER OF MOTORIST KILLED\"}]),\n",
    "            ]),\n",
    "        )\n",
    "    ])\n",
    "\n",
    "\n",
    "fig.update_layout(barmode='overlay')\n",
    "\n",
    "fig.update_xaxes(title_text='City')\n",
    "fig.update_yaxes(title_text='Count')\n",
    "fig.update_layout(title='Accidents district wise')\n",
    "fig.show()\n",
    "\n",
    "#push_viz(fig,'Accidentsdistrictwise2') #pushing the plot to plotly servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a53372e31ea64183af905ec9af6df95b",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### 3.3.3 Predictive model using accident data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "38fcc8e1014e45208ed3c60c32c5f38c",
    "deepnote_cell_height": 164.34375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "In this section we want to predict the number of people killed. By using the accident data along with temporal information, we can hopefully gain a deeper understanding as to what, if any, role the time plays in  accident patterns. First we will do a time series analysis where we would try to predict future crashes to  umber of crashes happening per day for a particular year helps us to identify the occurence of crashes & time factors like (day of week, hour ,season,etc) helps in analysing the behaviour of crashes with respect to time. In this section, we tried to built a machine learning regression model for predicting the crashes occuring per day per hour for forecasting in the future also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "afbb3eaf0df54ae4b518a708f01cfd1d",
    "deepnote_cell_height": 155.375,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     40.375
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 49,
    "execution_start": 1652207007088,
    "source_hash": "8682e46b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "districts = df[\"BOROUGH\"].dropna().unique()\n",
    "districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2d5d3a090e03464e9871c71fded03228",
    "deepnote_cell_height": 635,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 481,
    "execution_start": 1652207007147,
    "source_hash": "7fdce047",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_borough = df.groupby([\"BOROUGH\"])\n",
    "df_borough.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0ca98c7df241483c82c1a882ea85d68d",
    "deepnote_cell_height": 155.5625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### 3.3.3.1 Time Series Analysis\n",
    "\n",
    "\n",
    "The analysis of number of crashes happening per day for a particular year helps us to identify the occurence of crashes & time factors like (day of week, hour ,season,etc) helps in analysing the behaviour of crashes with respect to time. In this section, we tried to built a machine learning regression model for predicting the crashes occuring per day per hour for forecasting in the future also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ab932cd46b6e43fcb5f5ff847025d9e8",
    "deepnote_cell_height": 117,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 137,
    "execution_start": 1652207007407,
    "source_hash": "ecb77b5c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# converting to datetime format \n",
    "crash_data[\"CRASH DATE\"] = pd.to_datetime(crash_data[\"CRASH DATE\"],format=\"%Y-%m-%d %H:%M:%S.%f\");\n",
    "crash_data['Datetime'] = pd.to_datetime(crash_data['CRASH DATE']) + pd.to_timedelta(crash_data.pop('hourofday'), unit='H');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "e50f6a651042410694bdca8af612bf88",
    "deepnote_cell_height": 135,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 342,
    "execution_start": 1652207007557,
    "source_hash": "96138f3d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# groouping by crash per hour & creating the target column crash/hr\n",
    "crash_data= crash_data.groupby(['Datetime',]).count()[['CRASH TIME']].reset_index().rename(columns={'CRASH TIME':'crash/day'})\n",
    "crash_data['Datetime'] = pd.to_datetime(crash_data['Datetime']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "e63c797a62644849b852378035f22b4b",
    "deepnote_cell_height": 243,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 153,
    "execution_start": 1652207007899,
    "source_hash": "d394df1e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Adding season column and time features-holidays,lags,hour,month\n",
    "seasons = {12: 'Winter', 1: 'Winter', 2:'Winter', 3: 'Spring', 4: 'Spring', 5: 'Spring',6: 'Summer', 7: 'Summer', 8:'Summer', 9: 'Autumn', 10: 'Autumn', 11: 'Autumn'}\n",
    "us_holidays = holidays.US();\n",
    "crash_data['month'] = pd.DatetimeIndex(crash_data['Datetime']).month\n",
    "crash_data['Season'] = crash_data['month'].apply(lambda x: seasons[x])\n",
    "crash_data[\"holiday\"] = [1 if d in us_holidays else 0 for d in crash_data[\"Datetime\"]]\n",
    "crash_data['hour']=crash_data.Datetime.dt.hour\n",
    "crash_data['lag1']=crash_data['crash/day'].shift(1).bfill(0)\n",
    "crash_data = crash_data.set_index([\"Datetime\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "4f375ee2eed64d08a64ff0a5744dd544",
    "deepnote_cell_height": 135,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1652207008052,
    "source_hash": "5fd67bdc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#creating dummy variables to convert categorical columns\n",
    "crash_data = pd.concat([crash_data, pd.get_dummies(crash_data[\"hour\"], \"hour\")], axis=1)\n",
    "crash_data = pd.concat([crash_data, pd.get_dummies(crash_data[\"month\"], \"month\")], axis=1)\n",
    "crash_data = pd.concat([crash_data, pd.get_dummies(crash_data[\"Season\"], \"season\")], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2d08940363064058acdf296fa9f5b844",
    "deepnote_cell_height": 801,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 82,
    "execution_start": 1652207008053,
    "source_hash": "9735b617",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creation of model by spliting into train-test set\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "df = crash_data[['crash/day','holiday', 'hour', 'lag1', 'hour_0',\n",
    "       'hour_1', 'hour_2', 'hour_3', 'hour_4', 'hour_5', 'hour_6', 'hour_7',\n",
    "       'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12', 'hour_13',\n",
    "       'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18', 'hour_19',\n",
    "       'hour_20', 'hour_21', 'hour_22', 'hour_23', 'month_1', 'month_2',\n",
    "       'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8',\n",
    "       'month_9', 'month_10', 'month_11', 'month_12', 'season_Autumn',\n",
    "       'season_Spring', 'season_Summer', 'season_Winter']]\n",
    "# taking data from 2018 january to march for training and testing on prediction of crash/hr for April 2018 \n",
    "trainset_start =\"2018-01-01\"\n",
    "trainset_stop = \"2018-04-01\"\n",
    "testset_start = \"2018-04-01\"\n",
    "testset_stop = \"2018-04-30\"\n",
    "df_train = df.loc[((df.index >= trainset_start) & (df.index < trainset_stop))]\n",
    "df_test = df.loc[((df.index >= testset_start) & (df.index < testset_stop))]\n",
    "    \n",
    "X_train = df_train.drop([\"crash/day\"],axis=1)\n",
    "y_train = df_train.drop(['holiday', 'hour', 'lag1', 'hour_0',\n",
    "       'hour_1', 'hour_2', 'hour_3', 'hour_4', 'hour_5', 'hour_6', 'hour_7',\n",
    "       'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12', 'hour_13',\n",
    "       'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18', 'hour_19',\n",
    "       'hour_20', 'hour_21', 'hour_22', 'hour_23', 'month_1', 'month_2',\n",
    "       'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8',\n",
    "       'month_9', 'month_10', 'month_11', 'month_12', 'season_Autumn',\n",
    "       'season_Spring', 'season_Summer', 'season_Winter'],axis=1)\n",
    "\n",
    "X_test = df_test.drop([\"crash/day\"],axis=1)\n",
    "y_test = df_test.drop(['holiday', 'hour', 'lag1', 'hour_0',\n",
    "       'hour_1', 'hour_2', 'hour_3', 'hour_4', 'hour_5', 'hour_6', 'hour_7',\n",
    "       'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12', 'hour_13',\n",
    "       'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18', 'hour_19',\n",
    "       'hour_20', 'hour_21', 'hour_22', 'hour_23', 'month_1', 'month_2',\n",
    "       'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8',\n",
    "       'month_9', 'month_10', 'month_11', 'month_12', 'season_Autumn',\n",
    "       'season_Spring', 'season_Summer', 'season_Winter'],axis=1)\n",
    "#scaling of features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6dc5e98c6d0d47319711187021a27379",
    "deepnote_cell_height": 171,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 826,
    "execution_start": 1652207008136,
    "source_hash": "26562a34",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Using XGBRegressor to built the model\n",
    "regr = XGBRegressor()\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "y_train = regr.predict(X_train)\n",
    "y_pred = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ae1569203d7046ac9b6ec8edb84822ca",
    "deepnote_cell_height": 135,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1652207008963,
    "source_hash": "ff9d1494",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# defining mape for evaluation\n",
    "def mape(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f380b783b335488b877cbc8614546d82",
    "deepnote_cell_height": 153,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 21,
    "execution_start": 1652207008968,
    "source_hash": "657870fc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluation of model\n",
    "y_pred, y_train = np.clip(y_pred, 0, np.max(y_pred)), np.clip(y_train, 0, np.max(y_train))\n",
    "R2_train= r2_score(df_train['crash/day'], y_train)\n",
    "R2_test = r2_score(df_test['crash/day'], y_pred)\n",
    "Mape = mape(df_test['crash/day'], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f086a4a37fb64474a701b5699669f3c5",
    "deepnote_cell_height": 112.1875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 98,
    "execution_start": 1652207008990,
    "source_hash": "101e09bf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(Mape,R2_test,R2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ed08e8f00ff649c28cde88f46d51b20e",
    "deepnote_cell_height": 1287,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 96,
    "execution_start": 1652207008991,
    "source_hash": "1d53406d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to plot the predicted and observed values\n",
    "def prediction_plots1(reg_res,reg_set,R2_train, R2_test, Mape, regr_title):\n",
    "    ### Create a Dataframe that contains all the neccessary values\n",
    "    min_QQ=int(reg_set.min())-1\n",
    "    max_QQ=int(reg_set.max())+1\n",
    "    result=pd.DataFrame(data=reg_set.copy(),index=reg_set.index)\n",
    "    result.columns=['observed']\n",
    "    result['prediction']=reg_res\n",
    "    result['residuals']=result['observed']-result['prediction']\n",
    "    predicted=pd.DataFrame(reg_res,index=reg_set.index)\n",
    "    observed=pd.DataFrame(reg_set)\n",
    "    obs=observed.reset_index()\n",
    "    pre =predicted.reset_index()\n",
    "    # set up plotly subplots figure\n",
    "    fig_final = make_subplots(rows=1, cols=2, subplot_titles=(regr_title,  'Prediction values over test set values'))\n",
    "\n",
    "\n",
    "    ### FIGURE 1: Predicted vs Observed Values Plot\n",
    "    fig = px.scatter(result, x=\"observed\", y=\"prediction\", trendline_color_override='red', trendline=\"ols\", title=regr_title)\n",
    "          \n",
    "    \n",
    "    ### FIGURE 2: Predicted vs Observed Values Timeseries Plot\n",
    "    fig1 = go.Figure()     \n",
    "\n",
    "    fig1.add_trace(go.Scatter(\n",
    "        y=obs['crash/day'],\n",
    "        x=obs['Datetime'],\n",
    "        name='test set',\n",
    "        line = dict(color='green', dash='dash')\n",
    "        \n",
    "    ))\n",
    "    fig1.add_trace(go.Scatter(\n",
    "        y=pre[0],\n",
    "        x=pre['Datetime'],\n",
    "        name='predcited',\n",
    "        line = dict(color='royalblue')\n",
    "    ))\n",
    "\n",
    "    #set up plotly subplots\n",
    "    fig_final.add_trace(\n",
    "    fig.data[0],\n",
    "    row=1,\n",
    "    col=1,\n",
    "    )\n",
    "    fig_final.add_trace(\n",
    "    fig.data[1],\n",
    "    row=1,\n",
    "    col=1,\n",
    "    )\n",
    "    fig_final.add_trace(\n",
    "        fig1.data[0],\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "    fig_final.add_trace(\n",
    "        fig1.data[1],\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "    fig_final['layout']['xaxis']['title']='Observed'\n",
    "    fig_final['layout']['xaxis2']['title']='Date'\n",
    "    fig_final['layout']['yaxis']['title']='Predicted'\n",
    "    fig_final['layout']['yaxis2']['title']='Crashes per day'\n",
    "    fig_final.show()\n",
    "    push_viz(fig_final,'pre1') #pushing the plot to plotly servers\n",
    "\n",
    "    res= result['residuals'].sort_values()\n",
    "    return res, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "695b1292ca2b423e953bd4f88f3ec772",
    "deepnote_cell_height": 642,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     527
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3422,
    "execution_start": 1652207009088,
    "source_hash": "dcce6482",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting of observed v/s predicted and the time series plot for predicted values\n",
    "res, y_pred = prediction_plots1(y_pred,y_test,R2_train, R2_test, Mape, \"XGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4331c578f92e4294b7d44dc1b3861e6c",
    "deepnote_cell_height": 119.5625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "On the left side, the Quantile-Quantile (Q-Q) plots display the predicted over the observed values, the ones on the right overlay the predicted crashes with the observed ones over time. The performance of the XGboost model can be assessed more with the help of these plots which reflect the scores obtained in the previous part \n",
    "In the next section, cross validation process is followed to increase the robustness of the prediction models and determine other features that could improve it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ea50dee0bd92491ba937304771f5dcab",
    "deepnote_cell_height": 177.953125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### 3.3.3.2 CROSS VALIDATION \n",
    "\n",
    "In this section we are dividing the data set into different sets and validating the best model for prediction.\n",
    "hence we will be able to compare different models of regression and finalize to select the best model.\n",
    "To evaluate the performance of the different models it is not enough to compare the resulting scores from applying the prediction model defined above on the training sets. Cross validation is needed to develop more robust models and to determine the features that should be used for better predicting the pickups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "57fc45c7e90d403cac7137b322faa258",
    "deepnote_cell_height": 2511,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 12,
    "execution_start": 1652207012548,
    "owner_user_id": "66e22e42-0f29-4a8c-9873-b030de45371e",
    "source_hash": "9525d0f7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prediction(regr_type='linear', trainset_start = pd.to_datetime('2018-06-01 00:00:00'),\n",
    "               trainset_stop = pd.to_datetime('2018-08-01 00:00:00'),\n",
    "               testset_start = pd.to_datetime('2018-08-01 00:00:00'),\n",
    "               testset_stop =pd.to_datetime('2018-08-09 00:00:00')):\n",
    "    df = crash_data[['crash/day','holiday', 'hour', 'lag1', 'hour_0',\n",
    "       'hour_1', 'hour_2', 'hour_3', 'hour_4', 'hour_5', 'hour_6', 'hour_7',\n",
    "       'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12', 'hour_13',\n",
    "       'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18', 'hour_19',\n",
    "       'hour_20', 'hour_21', 'hour_22', 'hour_23', 'month_1', 'month_2',\n",
    "       'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8',\n",
    "       'month_9', 'month_10', 'month_11', 'month_12', 'season_Autumn',\n",
    "       'season_Spring', 'season_Summer', 'season_Winter']]\n",
    "    trainset_start =\"2018-01-01\"\n",
    "    trainset_stop = \"2018-04-01\"\n",
    "    testset_start = \"2018-04-01\"\n",
    "    testset_stop = \"2018-04-30\"\n",
    "    df_train = df.loc[((df.index >= trainset_start) & (df.index < trainset_stop))]\n",
    "    df_test = df.loc[((df.index >= testset_start) & (df.index < testset_stop))]\n",
    "        \n",
    "    X_train = df_train.drop([\"crash/day\"],axis=1)\n",
    "    y_train = df_train.drop(['holiday', 'hour', 'lag1', 'hour_0',\n",
    "        'hour_1', 'hour_2', 'hour_3', 'hour_4', 'hour_5', 'hour_6', 'hour_7',\n",
    "        'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12', 'hour_13',\n",
    "        'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18', 'hour_19',\n",
    "        'hour_20', 'hour_21', 'hour_22', 'hour_23', 'month_1', 'month_2',\n",
    "        'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8',\n",
    "        'month_9', 'month_10', 'month_11', 'month_12', 'season_Autumn',\n",
    "        'season_Spring', 'season_Summer', 'season_Winter'],axis=1)\n",
    "\n",
    "    X_test = df_test.drop([\"crash/day\"],axis=1)\n",
    "    y_test = df_test.drop(['holiday', 'hour', 'lag1', 'hour_0',\n",
    "        'hour_1', 'hour_2', 'hour_3', 'hour_4', 'hour_5', 'hour_6', 'hour_7',\n",
    "        'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12', 'hour_13',\n",
    "        'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18', 'hour_19',\n",
    "        'hour_20', 'hour_21', 'hour_22', 'hour_23', 'month_1', 'month_2',\n",
    "        'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8',\n",
    "        'month_9', 'month_10', 'month_11', 'month_12', 'season_Autumn',\n",
    "        'season_Spring', 'season_Summer', 'season_Winter'],axis=1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    models_dict= {'random': RandomForestRegressor(), 'linear': LinearRegression(), 'xgboost': XGBRegressor(),\n",
    "            'ridge': RidgeCV(), 'lasso':Lasso(), 'decisiontree':DecisionTreeRegressor(),\n",
    "             'bagging':BaggingRegressor(), 'adaboost':AdaBoostRegressor(), 'gradient':GradientBoostingRegressor()}\n",
    "    \n",
    "    titles_dict= {'random': 'Random Forest Regression Model', 'linear': 'Linear Regression', \n",
    "                  'xgboost': 'XGB Regression Model', 'ridge': 'RidgeCV Regression Model', \n",
    "                  'lasso':'Lasso', 'decisiontree':'Decision Tree Regression Model',\n",
    "                  'bagging':'Bagging Regression Model', 'adaboost':'AdaBoost Regression Model', \n",
    "                  }\n",
    "    if regr_type in models_dict.keys(): \n",
    "        regr = models_dict.get(regr_type)\n",
    "        regr_title = titles_dict.get(regr_type)\n",
    "\n",
    "    regr.fit(X_train, y_train)\n",
    "\n",
    "    y_train = regr.predict(X_train)\n",
    "    y_pred = regr.predict(X_test)\n",
    "    \n",
    "    \n",
    "    def mape(y_true, y_pred): \n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "\n",
    "    y_pred, y_train = np.clip(y_pred, 0, np.max(y_pred)), np.clip(y_train, 0, np.max(y_train))\n",
    "    \n",
    "    R2_train= r2_score(df_train['crash/day'], y_train)\n",
    "    R2_test = r2_score(df_test['crash/day'], y_pred)\n",
    "    Mape = mape(df_test['crash/day'], y_pred)\n",
    "\n",
    "    \n",
    "    return y_pred,y_test,R2_train, R2_test, Mape, regr_title, df_train, df_test\n",
    "def dates_crossvalidation(trainset_start,trainset_stop):\n",
    "    weeks_delta = (trainset_stop-trainset_start)/ np.timedelta64(1, 'W')\n",
    "    trainsets_starts=[]\n",
    "    trainsets_stops=[]\n",
    "    validationsets_starts=[]\n",
    "    validationsets_stops=[]\n",
    "    \n",
    "    for week in range(0,int(weeks_delta)-1): #loop through every trainset week except last one\n",
    "        trainsets_starts.append(trainset_start) #append the sets' start and end dates \n",
    "        trainsets_stops.append(trainset_start+timedelta(weeks=1))\n",
    "        validationsets_starts.append(trainset_start+timedelta(weeks=1)) \n",
    "        validationsets_stops.append(trainset_start+timedelta(weeks=2)) \n",
    "        trainset_start = trainset_start+timedelta(weeks=1) #updating the trainset_start shifting it by 1 week\n",
    "    return weeks_delta, trainsets_starts, trainsets_stops, validationsets_starts, validationsets_stops\n",
    "def perform_crossvalidation(regr_type, trainset_start, trainset_stop, testset_start, testset_stop):    \n",
    "    \n",
    "    weeks_delta, train_starts, train_stops, val_starts, val_stops= dates_crossvalidation(trainset_start,trainset_stop)\n",
    "    R2_trainset = []\n",
    "    R2_validationset =[]\n",
    "    Mape_validationset = []\n",
    "\n",
    "    \n",
    "    for (train_start, train_stop, val_start, val_stop) in zip(train_starts, train_stops, val_starts, val_stops):\n",
    "        #run the prediction \n",
    "        y_pred,y_test,R2_train, R2_test, Mape, regr_title, df_train, df_test  = prediction(regr_type=regr_type,\n",
    "               trainset_start = train_start,\n",
    "               trainset_stop = train_stop,\n",
    "               testset_start = val_start,\n",
    "               testset_stop = val_stop)\n",
    "        #store values in list\n",
    "        R2_trainset.append(R2_train)\n",
    "        R2_validationset.append(R2_test)\n",
    "        Mape_validationset.append(Mape)\n",
    "    return np.mean(R2_trainset), np.mean(R2_validationset), np.mean(Mape_validationset)\n",
    "def results_crossvalidation(models= ['random', 'linear', 'xgboost','ridge', 'lasso',\n",
    "                                    'decisiontree', 'bagging', 'adaboost']):\n",
    "    \n",
    "    crossvalidation_df = pd.DataFrame(columns = ['trainset_start', 'trainset_stop', 'testset_start', 'trainset_stop', \n",
    "                                 'R2_training_mean', 'R2_validation_mean', 'Mape_validation_mean','Model'])\n",
    "\n",
    "    for model in models:\n",
    "\n",
    "        for trainset_start in ['2018-01-01 00:00:00', '2018-02-01 00:00:00', '2018-03-01 00:00:00', '2018-04-01 00:00:00', \n",
    "                     '2018-06-05 00:00:00', '2018-07-01 00:00:00', '2018-08-01 00:00:00', '2018-09-01 00:00:00',\n",
    "                     '2018-10-01 00:00:00']: \n",
    "\n",
    "            trainset_start = pd.to_datetime(trainset_start, format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "            trainset_stop = trainset_start+ relativedelta(months=2)\n",
    "            testset_start = trainset_stop\n",
    "            testset_stop = testset_start + timedelta(days=9)\n",
    "\n",
    "            R2_train_mean, R2_val_mean, Mape_val_mean = perform_crossvalidation(model, trainset_start, \n",
    "                                                                                trainset_stop, testset_start, testset_stop)\n",
    "\n",
    "            crossvalidation_df.loc[len(crossvalidation_df)] = [trainset_start, trainset_stop, testset_start, testset_stop,\n",
    "                                                          R2_train_mean, R2_val_mean, Mape_val_mean,\n",
    "                                                              model]\n",
    "                                                          #R2_train_mean, R2_val_mean, Mape_val_mean]\n",
    "    grouped_results_df= crossvalidation_df.groupby(\"Model\").mean()\n",
    "    grouped_results_df= grouped_results_df.sort_values(by=\"Mape_validation_mean\")\n",
    "    \n",
    "    return grouped_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "562d6337c4004f96aeb70bef101d4438",
    "deepnote_cell_height": 554,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 132964,
    "execution_start": 1652207012575,
    "source_hash": "fd17af0f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# getting the comparison between models\n",
    "lag_results = results_crossvalidation()\n",
    "lag_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "44ef01e603b2465c8b5dee6dd731ea29",
    "deepnote_cell_height": 119.5625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "After doing cross-validation for different models XGB regressor performs as the best model\n",
    "With the current set up, the linear regression model is performing poorly. This is observed in the negative test set R2 score as well as the high MAPE. Contrarily, the XGboost seems to perform better with R2 scores close to 1 for both sets and MAPE of about 30%. This can also be observed in the table shown above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0c4eb84aee0a4442b0c4effec4f6e172",
    "deepnote_cell_height": 133.171875,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### 3.3.3.3 Adding Weather data features  \n",
    "\n",
    "Inorder to make prediction model more generic, data of weather features for the year 2018 has been incorporated with the data to make it more generic with addition of weather features like -  ('Temperature', 'Precipitation', 'Snow','Snow Depth', 'Wind Speed', 'Wind Direction','Visibility','Cloud Cover', 'Relative Humidity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "e5b17b68d1d7410a8ef27f333fe4eef9",
    "deepnote_cell_height": 430,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 67,
    "execution_start": 1652207145538,
    "source_hash": "32a76a8f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "weather_data = pd.read_csv(\"history_weather_data.csv\")\n",
    "weather_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "7ceba246c0e74694b35594fce210f715",
    "deepnote_cell_height": 135,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7,
    "execution_start": 1652207145604,
    "source_hash": "4a925a61",
    "tags": []
   },
   "outputs": [],
   "source": [
    "weather_data = weather_data[['Dates','Maximum Temperature', 'Minimum Temperature','Temperature', 'Precipitation','Snow Depth', 'Wind Speed','Visibility','Cloud Cover', 'Relative Humidity']];\n",
    "df_new = weather_data\n",
    "df_new[\"Dates\"] = pd.to_datetime(df_new[\"Dates\"]);\n",
    "df_new=df_new.rename(columns={'Dates': 'Datetime'});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ea3e3a99091e4d73a0bb0a74984775d8",
    "deepnote_cell_height": 135,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 9,
    "execution_start": 1652207145625,
    "source_hash": "db011fae",
    "tags": []
   },
   "outputs": [],
   "source": [
    "crash_data = crash_data.reset_index()\n",
    "#crash_data = crash_data.rename(columns={'Datetime': 'Dates'});\n",
    "crash_data=crash_data.merge(df_new, on=['Datetime'],how=\"inner\")\n",
    "crash_data = crash_data.set_index(\"Datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "39ab5e6506f7409eaf89c08897157798",
    "deepnote_cell_height": 1084.1875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 226,
    "execution_start": 1652207145674,
    "source_hash": "33c26e86",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = crash_data[['crash/day','holiday', 'hour', 'lag1', 'hour_0',\n",
    "    'hour_1', 'hour_2', 'hour_3', 'hour_4', 'hour_5', 'hour_6', 'hour_7',\n",
    "    'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12', 'hour_13',\n",
    "    'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18', 'hour_19',\n",
    "    'hour_20', 'hour_21', 'hour_22', 'hour_23', 'month_1', 'month_2',\n",
    "    'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8',\n",
    "    'month_9', 'month_10', 'month_11', 'month_12', 'season_Autumn',\n",
    "    'season_Spring', 'season_Summer', 'season_Winter', 'Temperature',\n",
    "    'Precipitation',  'Snow Depth', 'Wind Speed', 'Visibility', 'Cloud Cover', 'Relative Humidity']]\n",
    "trainset_start = \"2018-01-01\"\n",
    "trainset_stop =  \"2018-04-01\"\n",
    "testset_start =  \"2018-04-01\"\n",
    "testset_stop =   \"2018-04-30\"\n",
    "df_train = df.loc[((df.index >= trainset_start) & (df.index < trainset_stop))]\n",
    "df_test = df.loc[((df.index >= testset_start) & (df.index < testset_stop))]\n",
    "    \n",
    "X_train = df_train.drop([\"crash/day\"],axis=1)\n",
    "y_train = df_train.drop(['holiday', 'hour', 'lag1', 'hour_0',\n",
    "    'hour_1', 'hour_2', 'hour_3', 'hour_4', 'hour_5', 'hour_6', 'hour_7',\n",
    "    'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12', 'hour_13',\n",
    "    'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18', 'hour_19',\n",
    "    'hour_20', 'hour_21', 'hour_22', 'hour_23', 'month_1', 'month_2',\n",
    "    'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8',\n",
    "    'month_9', 'month_10', 'month_11', 'month_12', 'season_Autumn',\n",
    "    'season_Spring', 'season_Summer', 'season_Winter','Temperature',\n",
    "    'Precipitation',  'Snow Depth', 'Wind Speed', \n",
    "    'Visibility', 'Cloud Cover', 'Relative Humidity'],axis=1)\n",
    "\n",
    "X_test = df_test.drop([\"crash/day\"],axis=1)\n",
    "y_test = df_test.drop(['holiday', 'hour', 'lag1', 'hour_0',\n",
    "    'hour_1', 'hour_2', 'hour_3', 'hour_4', 'hour_5', 'hour_6', 'hour_7',\n",
    "    'hour_8', 'hour_9', 'hour_10', 'hour_11', 'hour_12', 'hour_13',\n",
    "    'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18', 'hour_19',\n",
    "    'hour_20', 'hour_21', 'hour_22', 'hour_23', 'month_1', 'month_2',\n",
    "    'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8',\n",
    "    'month_9', 'month_10', 'month_11', 'month_12', 'season_Autumn',\n",
    "    'season_Spring', 'season_Summer', 'season_Winter', 'Temperature',\n",
    "    'Precipitation', 'Snow Depth', 'Wind Speed',\n",
    "    'Visibility', 'Cloud Cover', 'Relative Humidity'],axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "regr = RandomForestRegressor()\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "y_train = regr.predict(X_train)\n",
    "y_pred = regr.predict(X_test)\n",
    "y_pred, y_train = np.clip(y_pred, 0, np.max(y_pred)), np.clip(y_train, 0, np.max(y_train))\n",
    "\n",
    "R2_train= r2_score(df_train['crash/day'], y_train)\n",
    "R2_test = r2_score(df_test['crash/day'], y_pred)\n",
    "Mape = mape(df_test['crash/day'], y_pred)\n",
    "\n",
    "print(\"Mape:\" ,Mape,\"R2_test:\",R2_test,\"R2_train:\",R2_train)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "28426750671c4f7480d04542d11ffd58",
    "deepnote_cell_height": 1269,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 23,
    "execution_start": 1652207145912,
    "source_hash": "765ce422",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prediction_plots2(reg_res,reg_set,R2_train, R2_test, Mape, regr_title):\n",
    "    ### Create a Dataframe that contains all the neccessary values\n",
    "    min_QQ=int(reg_set.min())-1\n",
    "    max_QQ=int(reg_set.max())+1\n",
    "    result=pd.DataFrame(data=reg_set.copy(),index=reg_set.index)\n",
    "    result.columns=['observed']\n",
    "    result['prediction']=reg_res\n",
    "    result['residuals']=result['observed']-result['prediction']\n",
    "    predicted=pd.DataFrame(reg_res,index=reg_set.index)\n",
    "    observed=pd.DataFrame(reg_set)\n",
    "    obs=observed.reset_index()\n",
    "    pre =predicted.reset_index()\n",
    "    # set up plotly subplots figure\n",
    "    fig_final = make_subplots(rows=1, cols=2, subplot_titles=(regr_title,  'Prediction values over test set values'))\n",
    "\n",
    "\n",
    "    ### FIGURE 1: Predicted vs Observed Values Plot\n",
    "    fig = px.scatter(result, x=\"observed\", y=\"prediction\", trendline_color_override='red', trendline=\"ols\", title=regr_title)\n",
    "          \n",
    "    \n",
    "    ### FIGURE 2: Predicted vs Observed Values Timeseries Plot\n",
    "    fig1 = go.Figure()     \n",
    "\n",
    "    fig1.add_trace(go.Scatter(\n",
    "        y=obs['crash/day'],\n",
    "        x=obs['Datetime'],\n",
    "        name='test set',\n",
    "        line = dict(color='green', dash='dash')\n",
    "        \n",
    "    ))\n",
    "    fig1.add_trace(go.Scatter(\n",
    "        y=pre[0],\n",
    "        x=pre['Datetime'],\n",
    "        name='predcited',\n",
    "        line = dict(color='royalblue')\n",
    "    ))\n",
    "\n",
    "    #set up plotly subplots\n",
    "    fig_final.add_trace(\n",
    "    fig.data[0],\n",
    "    row=1,\n",
    "    col=1,\n",
    "    )\n",
    "    fig_final.add_trace(\n",
    "    fig.data[1],\n",
    "    row=1,\n",
    "    col=1,\n",
    "    )\n",
    "    fig_final.add_trace(\n",
    "        fig1.data[0],\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "    fig_final.add_trace(\n",
    "        fig1.data[1],\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "    fig_final['layout']['xaxis']['title']='Observed'\n",
    "    fig_final['layout']['xaxis2']['title']='Date'\n",
    "    fig_final['layout']['yaxis']['title']='Predicted'\n",
    "    fig_final['layout']['yaxis2']['title']='Crashes per day'\n",
    "    fig_final.show()\n",
    "    push_viz(fig_final,'pre2') #pushing the plot to plotly servers\n",
    "\n",
    "    res= result['residuals'].sort_values()\n",
    "    return res, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "e90694a31a30425fb1ab06d3a297940a",
    "deepnote_cell_height": 642,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     527
    ],
    "deepnote_table_loading": false,
    "deepnote_table_state": {
     "filters": [],
     "pageIndex": 36,
     "pageSize": 10,
     "sortBy": []
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2923,
    "execution_start": 1652207145936,
    "source_hash": "5ab9afd3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prediction of model with addition of weather features for the month April 2018\n",
    "res, y_pred = prediction_plots2(y_pred,y_test,R2_train, R2_test, Mape, \"RF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3837fb40300c40d88dbc8d8e9c86f8c4",
    "deepnote_cell_height": 698.9375,
    "deepnote_cell_type": "markdown",
    "owner_user_id": "8303782f-c4e7-4c93-a630-a7cbabccfb98",
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "Visual Narra\n",
    "\n",
    "Narrative Structure\n",
    "\n",
    "Ordering: Linear\n",
    "Interactivity: Filtering / Selection / Search, Stimulating Default Views\n",
    "Messaging: Introductory text, Captions / Headlines, Annotations, Summary / Synthesis\n",
    "For the Narrative structure, linear ordering was thought to be the optimal choice, as a lot of information had to be communicated in text. Therefore, we assesed that an easy guiding of the reader would remove all potential confusions regarding where to look next, so that the reader's capacity was primarily spent on understanding the content. As touched upon above we chose a zig-zag layout to give this magazine style feeling and appealing look. For the interactive plots, interactivity switches between Filtering / Selection / Search and Stimulating Default views. In the Folium map we use a play button to animate the plot automatically, an option to adjust the fps for more convenient viewing and a scrolling bar for manually focusing on the specific points. Messaging is the use of text in order to provide observations and explanations about what is being presented, while interactivity gives the ability to the reader to manipulate the visualisations and focus on specific data of interest. The visualisations and respective interactivity were carefully chosen with the purpose of engaging the audience and enhance story discovery, but not detract from the main presented story. We started with an introductory text and the motivation, then used captions / headlines for each section and summaries before or after the plots to present the findings and the points of interest. Thereby, the reader will both be met with appealing and informative visualisations, while the captions / headlines will guide them through the story and help them with the broader perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "850ef5aa17a34369976f7f6f0cffcefd",
    "deepnote_cell_height": 355.515625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## <font color='green'>Part 4</font>: Genre\n",
    "<a id=part4></a>\n",
    "\n",
    "Selecting the right genre for a story depends on  on a variety of factors, including the complexity of the data, the complexity of the story, the intended audience and the intended medium.\n",
    "\n",
    "Narrative visualisation offers various ways to present your data-driven story. There are seven genres of Narrative Visualization, namely magazine style, annotated chart, partitioned poster, flow chart, comic strip, slide show, and video. \n",
    "\n",
    "In order to give the best experience for readers and enable them understand and investigate the analysis of data we have used a combination of magazine style and  annotated charts to narrate the learnings from the data,with additional interactivity and messaging elements. The reason we choose this is because it would help providing quick learning for the authority who wants to minimize the occurrence of the crashes in a particular location. Our visualizations will be supported by messaging and interactivity to give the reader a better understanding by exploring the data themselves.\n",
    "\n",
    "![genres](genres.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5ea8b9cd1a4846538fbb72592f7110f4",
    "deepnote_cell_height": 243.125,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**Narrative structure**\n",
    "* Visual Structuring: Consistent Visual Platform\n",
    "* Highlighting: Zooming\n",
    "* Transition Guidance: Animated Transitions\n",
    "\n",
    "The first category of narrative structure is visual structuring which is the overall structure of the narrative. In this category we have used Consistent Visual Platform where the content of each section changes but the general layout of the visual elements is consistent. For highlighting, we have used features such as Zooming and for Transition Guidance we have used Animated Transitions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bca849115afc477db02f00793c63a4b9",
    "deepnote_cell_height": 198.34375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**Visual Narrative**\n",
    "* Ordering :Linear\n",
    "* Interactivity: Hover Highlighting / Details - Selection\n",
    "* Messaging: Captions / Headlines - Annotations -  Comment Repitition-Introductory Text- Summary / Synthesis\n",
    "\n",
    "The first category of Visual Narrative is ordering, we have order the narrative in a linear order where the reader goes through a story telling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a3e0bf492a5c43c7a42e9c4b4a1a9742",
    "deepnote_cell_height": 86,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## <font color='green'>Part 5</font>: Visualizations\n",
    "<a id=part5></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d636d44a658e419789bf4ead1778f59f",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Explain the visualizations you've chosen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3abfc66d7c19448c955c1465bcb9f838",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "###  Why are they right for the story you want to tell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "91ac65c489094392a40d34288d78b56c",
    "deepnote_cell_height": 52.390625,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Different visualization plots were used for effective communication of our story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "16c6e42dfe554ca6977ae79b25969a62",
    "deepnote_cell_height": 46,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b92e255c1ba04269b9f22e5d943f3673",
    "deepnote_cell_height": 86,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## <font color='green'>Part 6</font>: Discussion\n",
    "<a id=part6></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e21b5addf1d94668b330313d840e4258",
    "deepnote_cell_height": 271.734375,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## <font color='green'>Part 7</font>: Contributions\n",
    "<a id=part7></a>\n",
    "\n",
    "1. Motivation: \n",
    "2. Basic Stats: \n",
    "3. Data Analysis:\n",
    "4. Genre: \n",
    "5. Visualizations:  \n",
    "6. Discussion:  \n",
    "Web page:  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "16e86dc6d5e94733aba8cf93ca59d5cc",
    "deepnote_cell_height": 46,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=40a4d6d1-a580-4519-81cc-ed43638bc8bb' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "74d53700-62cb-4d57-9d1b-45d46a3504d6",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
